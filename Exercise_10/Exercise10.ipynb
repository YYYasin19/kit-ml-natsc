{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5e74ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9177e12bf45acbd4e5d7e1c581723667",
     "grade": false,
     "grade_id": "cell-e12df818732a12f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise Sheet No. 10\n",
    "\n",
    "---\n",
    "\n",
    "> Machine Learning for Natural Sciences, Summer 2021, Jun.-Prof. Pascal Friederich, pascal.friederich@kit.edu\n",
    "> \n",
    "> Deadline: 29.06.2021, 8 am\n",
    "> \n",
    "> Tutor: patrick.reiser@kit.edu  \n",
    "> **Please ask questions in the forum and only contact the Tutor when there are issues with the grading**\n",
    "\n",
    "---\n",
    "**Topic**: This exercise sheet will introduce you to machine learning on graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7157e3f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7979ff4792ad6142cb65808ea1c2847a",
     "grade": false,
     "grade_id": "cell-df1f74f5c58193e8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Graph Theory\n",
    "\"In mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically. Graphs are one of the principal objects of study in discrete mathematics.\" ([wikipedia](https://en.wikipedia.org/wiki/Graph_theory))\n",
    "\n",
    "In one restricted but very common sense of the term, a graph is an ordered pair $G = ( V , E )$ comprising:\n",
    "\n",
    "* $V$, a set of vertices (also called nodes or points);\n",
    "* $E\\subseteq \\{\\{x,y\\}\\mid x,y\\in V\\;{\\textrm {and}}\\;x\\neq y\\}$, a set of edges (also called links or lines), which are unordered pairs of vertices (that is, an edge is associated with two distinct vertices).\n",
    "\n",
    "To avoid ambiguity, this type of object may be called precisely an undirected simple graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760abe09",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "814cffaa524a862849eaeff6bb29ddf6",
     "grade": false,
     "grade_id": "cell-0268a3c2d823ea3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Example of a graph\n",
    "G = nx.krackhardt_kite_graph()\n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac5b973",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22f909530f7e623051363b6ebd389b86",
     "grade": false,
     "grade_id": "cell-ae513ca0cdb0800f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Graph convolutional neural networks (GCN)\n",
    "\n",
    "Graph convolutional neural networks are a natural extenstion of CNNs for graph-structured data. A simple but efficient Graph neural network was introduced [\"Semi-Supervised Classification with Graph Convolutional Networks\"](https://arxiv.org/abs/1609.02907) by Kipf et al. (2016). Below is a description of the model and its applications taken from https://tkipf.github.io/graph-convolutional-networks/. We will implement this model and test it on some graph data.\n",
    "\n",
    "### GCNs Part I: Definitions\n",
    "\n",
    "Currently, most graph neural network models have a somewhat universal architecture in common. I will refer to these models as Graph Convolutional Networks (GCNs); convolutional, because filter parameters are typically shared over all locations in the graph (or a subset thereof as in [Duvenaud et al., NIPS 2015](https://proceedings.neurips.cc/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html)).\n",
    "\n",
    "For these models, the goal is then to learn a function of signals/features on a graph $G=(V,E)$ which takes as input:\n",
    "\n",
    "* A feature description $x_i$ for every node $i$; summarized in a $N\\times D$ feature matrix $X$ ($N$: number of nodes, $D$: number of input features)\n",
    "* A representative description of the graph structure in matrix form; typically in the form of an [adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix) $A$ (or some function thereof)\n",
    "\n",
    "and produces a node-level output $Z$ (an $N\\times F$ feature matrix, where $F$ is the number of output features per node). Graph-level outputs can be modeled by introducing some form of pooling operation (see, e.g. [Duvenaud et al., NIPS 2015](https://proceedings.neurips.cc/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html)).\n",
    "\n",
    "Every neural network layer can then be written as a non-linear function\n",
    "\n",
    "$$H^{(l+1)}=f(H^{(l)},A),$$\n",
    "\n",
    "with $H^{(0)}=X$ and $H^{(L)}=Z$ (or $z$ for graph-level outputs), $L$ being the number of layers. The specific models then differ only in how $f(⋅,⋅)$ is chosen and parameterized.\n",
    "\n",
    "### GCNs Part II: A simple example\n",
    "\n",
    "As an example, let's consider the following very simple form of a layer-wise propagation rule:\n",
    "\n",
    "$$f(H^{(l)},A)= \\sigma (AH^{(l)}W^{(l)}),$$\n",
    "\n",
    "where $W^{(l)}$ is a weight matrix for the $l$-th neural network layer and $\\sigma(⋅)$ is a non-linear activation function like the ReLU. Despite its simplicity this model is already quite powerful (we'll come to that in a moment).\n",
    "\n",
    "But first, let us address two limitations of this simple model: multiplication with $A$ means that, for every node, we sum up all the feature vectors of all neighboring nodes but not the node itself (unless there are self-loops in the graph). We can \"fix\" this by enforcing self-loops in the graph: we simply add the identity matrix to $A$.\n",
    "\n",
    "The second major limitation is that $A$ is typically not normalized and therefore the multiplication with $A$ will completely change the scale of the feature vectors (we can understand that by looking at the eigenvalues of $A$). Normalizing $A$ such that all rows sum to one, i.e. $D^{−1}A$, where $D$ is the diagonal node degree matrix, gets rid of this problem. Multiplying with $D^{−1}A$ now corresponds to taking the average of neighboring node features. In practice, dynamics get more interesting when we use a symmetric normalization, i.e. $D^{−\\frac{1}{2}} A D^{−\\frac{1}{2}}$ (as this no longer amounts to mere averaging of neighboring nodes). Combining these two tricks, we essentially arrive at the propagation rule introduced in [Kipf & Welling](https://arxiv.org/abs/1609.02907) (ICLR 2017): \n",
    "\n",
    "$$f(H^{(l)},A)=\\sigma \\, ( \\hat{D}^{−\\frac{1}{2}} \\hat{A} \\hat{D}^{−\\frac{1}{2}} H^{(l)}W^{(l)}),$$\n",
    "\n",
    "with $\\hat{A}=A+I$, where $I$ is the identity matrix and $\\hat{D}$ is the diagonal node degree matrix of $\\hat{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358957fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6201404279cbaa3ae444ab178cdd1ff2",
     "grade": false,
     "grade_id": "cell-f648a16e58e30317",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we will take a closer look at how this type of model operates on a very simple example graph: Zachary's karate club network (make sure to check out the [Wikipedia article](https://en.wikipedia.org/wiki/Zachary%27s_karate_club)!). A social network of a karate club was studied by Wayne W. Zachary for a period of three years from 1970 to 1972. The network captures 34 members of a karate club, documenting links between pairs of members who interacted outside the club. During the study a conflict arose between the administrator \"John A\" and instructor \"Mr. Hi\" (pseudonyms), which led to the split of the club into two. Half of the members formed a new club around Mr. Hi; members from the other part found a new instructor or gave up karate. Based on collected data Zachary correctly assigned all but one member of the club to the groups they actually joined after the split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d910d53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59195687e7f8a335e793c36aad16c4ac",
     "grade": false,
     "grade_id": "cell-3fa3bc6db7884ffa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()\n",
    "labels = [G.nodes()[i]['club'] for i in range(nx.number_of_nodes(G))]\n",
    "print(\"Example of labels[5:15] :\",labels[5:15])\n",
    "labels_onehot = [0 if x=='Mr. Hi' else 1 for x in labels]\n",
    "labels_color = ['green' if x=='Mr. Hi' else \"red\" for x in labels]\n",
    "nx.draw_circular(G, with_labels=True, node_color = labels_color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766c9f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "034f072c1c124156072eee16b7e02124",
     "grade": false,
     "grade_id": "cell-ba3d1f3df6681688",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "# Adjacency matrix as sparse matrix\n",
    "# Note: Larger graphs can not be stored in memory anymore\n",
    "A = nx.adjacency_matrix(G)\n",
    "print(A.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6bde9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01c80d57f4e3f6800464170be599b8da",
     "grade": false,
     "grade_id": "cell-c447e57e44fd3c72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 10.1 Normalized adjacency matrix\n",
    "Compute the prescaled adjacency matrix $A_s = \\hat{D}^{−\\frac{1}{2}} \\hat{A} \\hat{D}^{−\\frac{1}{2}}$, with $\\hat{A}=A+I$, where $I$ is the identity matrix and $\\hat{D}$ is the diagonal node degree matrix of $\\hat{A}$.\n",
    "You have to implement ``compute_scaled_DAD`` using scipy's sparse matrix functions, such as `.sum`, `sp.diag`, `sp.eye`, `.shape`, `.transpose` and `.dot`. It is okay to compute the power of the row- and column sum with `numpy`. You may have to check for `np.inf` here when taking $\\hat{D}^{−\\frac{1}{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009389e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "725a94c0092064384fdf5839b28b503c",
     "grade": false,
     "grade_id": "task_compute_scaled_DAD",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_scaled_DAD(adj):\n",
    "    \"\"\"Compute scaled adjacency matrix D^-0.5*(A + I)*D^-0.5\n",
    "    with the degree matrix D of (A+I).\n",
    "    \n",
    "    Args:\n",
    "        adj (scipy.sparse): Sparse matrix representation of A with shape (N, N)\n",
    "    \n",
    "    Returns:\n",
    "        sp.coo.coo_matrix: Sparse matrix representation of D^-0.5*(A + I)*D^-0.5\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return a_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d628e13",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f30347e3704932a97ba9c822d056595",
     "grade": false,
     "grade_id": "cell-8b18321511d58e0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test on the Karate Club network\n",
    "Ascaled = compute_scaled_DAD(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27c002",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3175e68e49c0fc95a9cdbce03d3e6e8d",
     "grade": true,
     "grade_id": "test_A_scaled",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Ascaled, sp.coo.coo_matrix)\n",
    "assert np.amax(np.abs(Ascaled.tocoo().toarray()[:5,:5]-np.array(\n",
    "    [[0.05882353, 0.0766965 , 0.07312724, 0.09166985, 0.12126781],\n",
    "       [0.0766965 , 0.1       , 0.09534626, 0.11952286, 0.        ],\n",
    "       [0.07312724, 0.09534626, 0.09090909, 0.11396058, 0.        ],\n",
    "       [0.09166985, 0.11952286, 0.11396058, 0.14285714, 0.        ],\n",
    "       [0.12126781, 0.        , 0.        , 0.        , 0.25      ]]))) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965af1cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e80aa395c78d4ec286f6755da9edbdd5",
     "grade": false,
     "grade_id": "cell-51d74fb60a8c4273",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we implement the model in TensorFlow, where the node features and the pre-scaled adjacency matrix are model inputs. The actual convolution realized by a (sparse) matrix multiplication is given in the layer below: `GCNConvolution`. This means the scaling does not to be performed within the model but can be done beforehand via `compute_scaled_DAD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8e423",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64b2fa93aab2dd2bcc954324220b7677",
     "grade": false,
     "grade_id": "cell-6761c99078bb6033",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class GCNConvolution(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(GCNConvolution, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        adj, x = inputs \n",
    "        return tf.sparse.sparse_dense_matmul(adj, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155c7d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "853cd0396fcc940744c8de39c915051d",
     "grade": false,
     "grade_id": "cell-42be802a92429c0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We set up the model in the functional API for TensorFlow Keras. You can read about this here: https://www.tensorflow.org/guide/keras/functional. A skeleton of the intended model is shown below. You have to add 3 layers of convolution. For the weights $W^{(l)}$ you can simply use `tf.keras.layers.Dense` with `linear` activation and without any `use_bias=False` and for the non-linearity after the matrix multiplication you can use `tf.keras.layers.Activation`. For activation you can use ``\"relu\"`` and the dimension of the kernel is given already below as `hidden_dim`. We wrote some pseudo-code in the section, where you have to implement the model-part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa4b4c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f85a1ad0e9d2c7d51d01a6ea67d6996",
     "grade": false,
     "grade_id": "task_GCN_sparse_implement",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Model properties\n",
    "hidden_dim = 34\n",
    "target_dim = 2\n",
    "depth = 3\n",
    "# Model definition\n",
    "input_adj = tf.keras.layers.Input(shape=(34,), name='adj_input', dtype=\"float32\", sparse=True) # Scaled Adjacency matrix\n",
    "input_feat = tf.keras.layers.Input(shape=(34,), name='node_input', dtype=\"float32\") # Node features\n",
    "x = input_feat\n",
    "for i in range(depth):\n",
    "    # Pseudo-Code of the model \"x = D^-0.5*(A + I)*D^-0.5 * W *x\"\n",
    "    #\n",
    "    # x = W*x (via tf.keras.layers.Dense(hidden_dim))\n",
    "    # x = A_scaled * x (via GCNConvolution())\n",
    "    # x = sigma(x) (via tf.keras.layers.Activation('relu'))\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "out_classes = tf.keras.layers.Dense(target_dim, activation='softmax')(x)\n",
    "model = tf.keras.models.Model(inputs=[input_adj, input_feat], outputs=out_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e639b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c26c1253d3d94ee9da55207da547c979",
     "grade": false,
     "grade_id": "cell-6a94b5109e1b7e66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will train the model above on a semi-supervised learning procedure, meaning that we will train the model on the Karate club network with a couple of nodes cloaked or covered and then test if the network can predict the right assignment of the unkown students. Since the network does not have features `X`, we will simply assume `X=I`. Check the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10233f66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96e77bed6f61c23a7d244efa1f55f74f",
     "grade": false,
     "grade_id": "cell-0242d3b68ef1ea35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Features and labels\n",
    "X = np.eye(34)\n",
    "Y = np.array([[1,0] if x=='Mr. Hi' else [0,1] for x in labels])\n",
    "As = Ascaled.tocsr().sorted_indices().tocoo()\n",
    "# Train validatoin mask to cover nodes in the training\n",
    "index_karate = np.arange(34)\n",
    "ind_train, ind_val = train_test_split(index_karate, test_size=0.25, random_state=42)\n",
    "val_mask = np.zeros_like(index_karate)\n",
    "train_mask = np.zeros_like(index_karate)\n",
    "val_mask[ind_val] = 1\n",
    "train_mask[ind_train] = 1\n",
    "#Draw the graphs\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "nx.draw_circular(G, with_labels=True, node_color = [x if train_mask[i]==1 else 'black' for i,x in enumerate(labels_color)])\n",
    "plt.title(\"Training Graph\")\n",
    "plt.subplot(122)\n",
    "nx.draw_circular(G, with_labels=True, node_color = [x if val_mask[i]==1 else 'black' for i,x in enumerate(labels_color)])\n",
    "plt.title(\"Validation Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1856e4a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dff324e69578f25f3b932cafd21d319e",
     "grade": false,
     "grade_id": "cell-da26be3c5c14fdca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can use TensorFlow Keras API to train the model with some modifications. This is a somewhat hacky solution and not really as the Keras training API is intended. We directly insert the sparse matrix and Keras will think of the nodes as samples. We therefore have to fix the `batch_size=34`. Also we must prevent shuffling of the nodes (that will destroy our graph) and set `shuffle=False`. The covering of validation node labels can be realized with a `sample_weight=train_mask` that sets the validation nodes to zero in the loss (since nodes correspond to samples in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9277f01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f27c31457d8847fa55c252c0d03df989",
     "grade": false,
     "grade_id": "cell-91c6de5429da3aac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compile model with optimizer and loss\n",
    "learning_rate_start = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_start)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              weighted_metrics=['categorical_accuracy'])\n",
    "hist = model.fit([As, X], Y,\n",
    "                 epochs=100,\n",
    "                 batch_size=34,\n",
    "                 verbose=1,\n",
    "                 shuffle=False,  # Since we do not really have batches, nodes must not be shuffled\n",
    "                 sample_weight=train_mask  # Important to hide values from \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297068a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18a7d3f2e35805c699fc22d8b9143812",
     "grade": false,
     "grade_id": "cell-f8d984ab33dd02a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "val_loss = model.evaluate([As, X], Y, batch_size=34, sample_weight=val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fafba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd6368e637a97354e7f32f90604b775d",
     "grade": false,
     "grade_id": "cell-c1b393c7c659c9ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pred = model.predict([As, X], batch_size=34)\n",
    "pred_val = pred[np.array(val_mask,dtype=\"bool\")]\n",
    "pred_train = pred[np.array(train_mask,dtype=\"bool\")]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(pred_train[:,0],pred_train[:,1], s=100, alpha=0.5, c=[x for i,x in enumerate(labels_color) if train_mask[i]==1])\n",
    "plt.plot((0,1),c='black')\n",
    "plt.title(\"Training Prediction\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(pred_val[:,0],pred_val[:,1], s=100, alpha=0.5, c=[x for i,x in enumerate(labels_color) if val_mask[i]==1])\n",
    "plt.title(\"Validation Prediction\")\n",
    "plt.plot((0,1),c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a93ed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70a2095c6800d13fff233b84e94ab307",
     "grade": true,
     "grade_id": "test_GCN_sparse_acc",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert val_loss[-1] > 0.8\n",
    "assert hist.history['categorical_accuracy'][-1] > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bffdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eeaef8aef36561d5d5a2bf66d39f7832",
     "grade": false,
     "grade_id": "cell-c8523643ca5d8313",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Why is it not suprising that the training accuracy approaches 1.0 ? But why is it interesting that the valdiation accuracy is high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4b99c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b59339fda7581058f8d87bd70fdf2fb",
     "grade": false,
     "grade_id": "task_answer_GCN",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_training_acc = None\n",
    "answer_validation_acc = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a6657",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f317589a20401ebe0f3da83a19df6d52",
     "grade": true,
     "grade_id": "test_answer_GCN",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(answer_training_acc, str)\n",
    "assert isinstance(answer_validation_acc, str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b99174",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bc1a72ebabe08661f8d4f6e0dd2a166",
     "grade": false,
     "grade_id": "cell-53cb7b55d6253a1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 10.2 Molecules\n",
    "Molecules can also be represented as graphs. In this case we have many small(er) graphs and are intereseted in graph-embedding or graph-classification rather than node-embedding or -classification. For this part we use the MUTAG dataset  that classifies a set of small molecules as mutagenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712be8e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "220af65ff75a19bba41610e64e65e6f7",
     "grade": false,
     "grade_id": "cell-281cbb016a682e34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "data_url = \"https://ls11-www.cs.tu-dortmund.de/people/morris/graphkerneldatasets/MUTAG.zip\"\n",
    "if not os.path.exists(\"MUTAG.zip\"):\n",
    "    print(\"Downloading dataset ...\")\n",
    "    r = requests.get(data_url) \n",
    "    _ = open(\"MUTAG.zip\",'wb').write(r.content)\n",
    "    archive = zipfile.ZipFile(\"MUTAG.zip\", \"r\")\n",
    "    archive.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28788a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b0a52927ed37a41348ef3d6b0a8d3f5",
     "grade": false,
     "grade_id": "cell-d5eaad9d551c9823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_mutag_text(filename):\n",
    "    mutag_txt = []\n",
    "    open_file = open(filename, \"r\")\n",
    "    for lines in open_file.readlines():\n",
    "        idxs = lines.strip().split(',')\n",
    "        idxs = [int(x) for x in idxs]\n",
    "        mutag_txt.append(idxs)\n",
    "    open_file.close()\n",
    "    return np.array(mutag_txt, dtype=\"int\")\n",
    "# Read all mutag files\n",
    "mutag_A = read_mutag_text(\"MUTAG/MUTAG_A.txt\")\n",
    "mutag_E = read_mutag_text(\"MUTAG/MUTAG_edge_labels.txt\")\n",
    "mutag_G = read_mutag_text(\"MUTAG/MUTAG_graph_indicator.txt\")\n",
    "mutag_N = read_mutag_text(\"MUTAG/MUTAG_node_labels.txt\")\n",
    "mutag_L = read_mutag_text(\"MUTAG/MUTAG_graph_labels.txt\")\n",
    "print(\"Shape of\", \"A:\",mutag_A.shape, \", Edges:\", mutag_E.shape,\", Graph-ID:\", mutag_G.shape,\", Nodes:\", mutag_N.shape,\", Graph-label:\", mutag_L.shape)\n",
    "# Want to start index from zero and not from 1\n",
    "mutag_A = mutag_A-1\n",
    "mutag_G = mutag_G-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd65660a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c803d1d48d8fb4f5a395304f84799b95",
     "grade": false,
     "grade_id": "cell-565522fc577b76fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Separate the molecular graphs:\n",
    "atom_code = [\"C\", \"N\", \"O\", \"F\", \"I\", \"Cl\", \"Br\"]\n",
    "atom_color_map = {\"C\": \"grey\", \"N\": \"green\", \"O\": \"blue\", \"F\" : \"purple\", \"I\" : \"yellow\", \"Cl\":\"pink\", \"Br\": \"orange\"}\n",
    "all_graphs = nx.Graph()\n",
    "for i in range(len(mutag_N)):\n",
    "    one_hot_atom_embedding = [0] * len(atom_code) # There are 7 elements in the dataset\n",
    "    one_hot_atom_embedding[mutag_N[i][0]] = 1\n",
    "    str_atom_name=atom_code[mutag_N[i][0]]\n",
    "    all_graphs.add_node(i, \n",
    "                        features=one_hot_atom_embedding, \n",
    "                        atom_name=str_atom_name)\n",
    "for i in range(len(mutag_A)):\n",
    "    one_hot_bond_embedding = [0] * 4 # There are 3 bond types in the dataset\n",
    "    one_hot_bond_embedding[mutag_E[i][0]] = 1\n",
    "    all_graphs.add_edge(mutag_A[i][0], mutag_A[i][1], \n",
    "                        features=one_hot_bond_embedding)\n",
    "graphs = []\n",
    "nodes = np.arange(0,len(mutag_N),1)\n",
    "for g in range(0,np.amax(mutag_G)):\n",
    "    graphs.append(all_graphs.subgraph(nodes[mutag_G[:,0]==g]).to_directed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8f1f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bde493a5db5ca72ac93fbd4f2ea52755",
     "grade": false,
     "grade_id": "cell-c06bedd8bd8c422e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_sub_graph(gri_plot):\n",
    "    nx.draw(graphs[gri_plot], labels={n: (graphs[gri_plot].nodes()[n]['atom_name']) for n in graphs[gri_plot].nodes()},\n",
    "        node_color = [atom_color_map[graphs[gri_plot].nodes()[n]['atom_name']] for n in graphs[gri_plot].nodes()])\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(231)\n",
    "plot_sub_graph(2)\n",
    "plt.subplot(232)\n",
    "plot_sub_graph(75)\n",
    "plt.subplot(233)\n",
    "plot_sub_graph(23)\n",
    "plt.subplot(234)\n",
    "plot_sub_graph(101)\n",
    "plt.subplot(235)\n",
    "plot_sub_graph(21)\n",
    "plt.subplot(236)\n",
    "plot_sub_graph(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dd393",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb6d883b3bf5915c9f8d41d3b18fb466",
     "grade": false,
     "grade_id": "cell-b4400beb2d6a696e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "As = [nx.adjacency_matrix(graphs[i]) for i in range(len(graphs))]\n",
    "Xs = [np.array([g.nodes()[n]['features'] for n in g.nodes()]) for g in graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8f946",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d884a67e90c6f421da26c8d1922969d7",
     "grade": false,
     "grade_id": "cell-52bf36f26f293aa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we scale the matrix and already cast it to a dense array\n",
    "# If you did not solve previous part you can do it with numpy functions here.\n",
    "As = [compute_scaled_DAD(a).toarray() for a in As]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ce48d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf7ef5cea2cbb818cb3be566c9070360",
     "grade": false,
     "grade_id": "cell-4c4212d700cbfb83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To put multiple graphs of different size in a single tensor, we will use padding here. That means we simply fill up the tensor with zeros. However, since zeros can cause non-zero output (e.g. bias), a mask to ignore these values has to be added to the model. Keras also has masking capabilities for example for RNNs/LSTM and Masking passing between layers but we will do it manually \"by hand\" or explicitly here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb449e0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b845e285bb0289a2872fbd4f6f32fbeb",
     "grade": false,
     "grade_id": "task_padding_A_X",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Apadded = np.zeros((187,28,28))\n",
    "Amask = np.zeros((187,28,28), dtype=\"bool\")\n",
    "Xpadded = np.zeros((187,28,7))\n",
    "Xmask = np.zeros((187,28,1), dtype=\"bool\")\n",
    "for i,a in enumerate(As):\n",
    "    # Fill Apadded, Amask with correct values\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "for i,x in enumerate(Xs):\n",
    "    # Fill Xpadded, Xmask with correct values\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "labels = mutag_L\n",
    "labels[labels<0]=0 # labels are strangely -1, 1 so we set to 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6197313",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74a7b90c358e6796d246c77d5d0db660",
     "grade": true,
     "grade_id": "test_padding_AX",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.amax(np.abs(Apadded[75,:5,:5]- np.array(\n",
    "    [[0.33333333, 0.33333333, 0.        , 0.        , 0.        ],\n",
    "       [0.33333333, 0.33333333, 0.28867513, 0.        , 0.        ],\n",
    "       [0.        , 0.28867513, 0.25      , 0.25      , 0.        ],\n",
    "       [0.        , 0.        , 0.25      , 0.25      , 0.28867513],\n",
    "       [0.        , 0.        , 0.        , 0.28867513, 0.33333333]]))) < 1e-4\n",
    "assert np.amax(np.abs(Apadded[75,10:,10:]))< 1e-4\n",
    "assert np.amax(np.abs(Xpadded[75,10:,:]))< 1e-4\n",
    "assert np.amax(np.abs(Xpadded[75,:10,0]-np.array([1., 1., 1., 1., 1., 0., 0., 0., 0., 1.])))< 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2baaa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64339b19375529030f8cb94ad3ec9bb4",
     "grade": false,
     "grade_id": "cell-c32f913f98b651f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The model can be defined similar to the previous task (skeleton below). However, this time you can do the matrix multiplication with `tf.keras.layers.Dot` and applying the mask with `tf.keras.layers.Multiply`. We will derive a graph-embedding by simply averaging all the node-embeddings of the last layer. For this purpose, we use the `Pooling` layer after the graph convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90d362",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ecbb09ede050f902422fd0efe407fae",
     "grade": false,
     "grade_id": "cell-914f10c69414ea7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class Pooling(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(Pooling, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        return tf.math.reduce_mean(inputs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b0b8a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a70d358c069c0b107974dff488842dcd",
     "grade": false,
     "grade_id": "task_GCN_padd_imp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Model properties\n",
    "hidden_dim = 32\n",
    "target_dim = 1\n",
    "depth = 3\n",
    "# Model definition\n",
    "input_adj = tf.keras.layers.Input(shape=(28,28), name='adj_input', dtype=\"float32\")\n",
    "input_x = tf.keras.layers.Input(shape=(28,7), name='atom_input', dtype=\"float32\")\n",
    "input_x_mask = tf.keras.layers.Input(shape=(28,1), name='atom_mask', dtype=\"float32\")\n",
    "x = input_x\n",
    "for i in range(depth):\n",
    "    # Pseudo-Code of the model \"x = D^-0.5*(A + I)*D^-0.5 * W *x\"\n",
    "    #\n",
    "    # x = W*x (via tf.keras.layers.Dense(hidden_dim))\n",
    "    # x = Mask(x) (via tf.keras.layers.Multiply())\n",
    "    # x = A_scaled * x (via tf.keras.layers.Dot() think about the axes argument)\n",
    "    # x = sigma(x) (via tf.keras.layers.Activation('relu'))\n",
    "    # x = Mask(x) (via tf.keras.layers.Multiply())\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "x_pool = Pooling()(x)\n",
    "out_classes = tf.keras.layers.Dense(target_dim, activation='sigmoid')(x_pool)\n",
    "model = tf.keras.models.Model(inputs=[input_adj, input_x, input_x_mask], outputs=out_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2b4f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d9ea7e391ff135aab68da5166918d1e",
     "grade": false,
     "grade_id": "cell-5f962097ce9e72af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Compile model with optimizer and loss\n",
    "learning_rate_start = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate_start)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              weighted_metrics=['accuracy'])\n",
    "hist = model.fit([Apadded, Xpadded, Xmask], labels,\n",
    "                 epochs=500,\n",
    "                 batch_size=64,\n",
    "                 verbose=1,\n",
    "                 shuffle=True, \n",
    "                 validation_split=0.15\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba9255",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00115bfc8c0be3e4e25d3ead1c1297a9",
     "grade": false,
     "grade_id": "cell-01c35b2f7f232988",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(hist, validation_freq=1,scale=1):    \n",
    "    plt.figure()\n",
    "    for key, loss in hist.history.items():\n",
    "        np_loss=np.array(loss)\n",
    "        if \"val\" in key:\n",
    "            plt.plot(np.arange(np_loss.shape[0])*validation_freq+validation_freq,np_loss, label=key)\n",
    "        else:\n",
    "            plt.plot(np.arange(np_loss.shape[0]), np_loss, label=key)\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss ')\n",
    "    plt.title('Loss vs. epochse')\n",
    "    plt.legend(loc='upper right', fontsize='x-small')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(hist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc46fbf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4fa02b468a923584f9d978f15a79a12f",
     "grade": true,
     "grade_id": "test_MUTAG_model",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hist.history['accuracy'][-1]> 0.7\n",
    "assert hist.history['val_accuracy'][-1]> 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
