{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6a56d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f75836a0f757399dc4b54cca499979e",
     "grade": false,
     "grade_id": "cell-568f36b1a4156bca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise Sheet No. 11\n",
    "\n",
    "---\n",
    "\n",
    "> Machine Learning for Natural Sciences, Summer 2021, Jun.-Prof. Pascal Friederich, pascal.friederich@kit.edu\n",
    "> \n",
    "> Deadline: 05.07.2021, 8 am\n",
    "> \n",
    "> Tutor: luca.torresi@kit.edu  \n",
    "> **Please ask questions in the forum and only contact the Tutor when there are issues with the grading**\n",
    "\n",
    "---\n",
    "**Topic**: This exercise sheet will introduce you to generative models in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b067d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9d2adc230910e077c7c23cbc304dd8e",
     "grade": false,
     "grade_id": "cell-053311f321fcb02d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Variational AutoEncoders\n",
    "\n",
    "In this exercise we will work on Variational AutoEncoders, first introduced by Welling and Kingma in a popular paper published in 2014 (https://arxiv.org/pdf/1312.6114.pdf).\n",
    "We will implement a vanilla version of VAE, with encoder and decoder composed of fully connected layers only, and try to use it for an anomaly detection task. We will then see how we can impose nice properties on the latent representation of data produced by the network simply augmenting it with a linear regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaceed3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be6abaa5f258848762cf880c6a0671f3",
     "grade": false,
     "grade_id": "cell-99005ee5eaa17570",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from collections import Counter\n",
    "\n",
    "from typing import Optional\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from Utils import get_data, AbstractTrainer, plot_reconstr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf591d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a9bfef5f4ccfe791b21418bbcb70164",
     "grade": false,
     "grade_id": "cell-bf89fd1ba7509706",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's take a look at our data we will work on.\n",
    "\n",
    "The dataset consists of ECG segments of healthy patients and patients suffering from premature ventricular contraction (PVC). Segments were padded/cut to measure 350 time steps each.\n",
    "\n",
    "In the first part of the exercise we will implement a detector for anomalous hearbeats, thus our training and validation sets will consist of normal heartbeats only while in the test set we will have both normal and PVC ECGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e9dd6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c86b3911854686e161c3cadbef969815",
     "grade": false,
     "grade_id": "cell-6c351878ab0d4d10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(101)\n",
    "train, val, test = get_data(batch_size=1)\n",
    "print(f'Dimension of \\n\\t train set : {len(train)} data points \\n\\t validation set : {len(val)} data points \\n\\t test set : {len(test)} data points')\n",
    "counts = Counter()\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True)\n",
    "for batch in test:\n",
    "    if sum(counts.values())==6:\n",
    "            break\n",
    "    data = batch['data']\n",
    "    label = batch['labels']\n",
    "    if label==0 and counts[0]<3:\n",
    "        axs[0, counts[0]].plot(np.arange(350), data.squeeze())\n",
    "        counts[0] += 1\n",
    "    if label==1 and counts[1]<3:\n",
    "        axs[1, counts[1]].plot(np.arange(350), data.squeeze())\n",
    "        counts[1] += 1\n",
    "axs[0,0].set_ylabel('Normal')\n",
    "axs[1,0].set_ylabel('PVC')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a0b02",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3f5114dbbe5c05e2414787873b17f7b",
     "grade": false,
     "grade_id": "cell-d4aeeb6e41141a77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "An autoencoder is an unsupervised representation learning algorithm based on neural networks that is used to learn efficient codings of unlabeled data. In particular, by imposing a bottleneck in the encoding layer of the network we can extract a compressed representation of the original input leveraging the structure present in it, i.e. the correlations among input features. \n",
    "\n",
    "In an AE input data is converted into a deterministic encoding vector, i.e. for each of the latent features the encoder outputs a single value. VAEs are an extension of AE int the sense that instead of performing a single value estimation of the latent vector they produce a probabilistic distribution $q_{\\phi}(z|x)$ in the latent space. This is done through a variational bayesian inference: an approximation of the distribution is picked from some tractable family and then this is made as close as possible to the true posterior. In particular in the original formulation of VAE the prior imposed on the latent representation is a standard Gaussian and the mean field approximation is used, i.e. the latent variables are considered as independent from each other.\n",
    "\n",
    "Being the latent representation of our data a probabilistic distribution, to reconstruct input data we have to sample points from it $z$~$q_{\\phi}(z|x)$. The fact that one of the nodes in the computational graph is a random node would create a problem in the backpropagation step because the gradient would not be able to 'flow' through it.\n",
    "\n",
    "One of the key elements in the VAE architecture is the so called 'reparameterization trick', that allow us to express a distribution $q_{\\phi}(z|x)$ as a two-step generative process:\n",
    "1. sample a noise variable $\\epsilon$ from a simple distribution:\n",
    "2. apply a deterministic transformation $g_{\\phi}(\\epsilon,x)$ that maps the random noise into a more complex distribution.\n",
    "\n",
    "\n",
    "In the case of Gaussian distributions the reparametrization trick is particularly simple: instead of writing $z$∼$q_{\\phi}(z|x)=\\mathcal{N}(\\mu_{\\phi}, \\Sigma_{\\phi})$ we can just write $z=g_{\\phi}(\\epsilon, x)= \\mu_{\\phi} +  \\sigma_{\\phi}*\\epsilon$,  where $\\epsilon$∼$\\mathcal{N}(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c52f9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9572bbb0be3b16da18de453cf0246309",
     "grade": false,
     "grade_id": "cell-18abb1cd4791623c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the following a partial implementation of a simple VAE with MLPs as encoder and decoder is provided. Complete the functions, paying attention to the fact that the encoder has to produce the logarithm of the standard deviation of each of the latent variables (this is done to ensure that the standard deviation is always positive).\n",
    "\n",
    "Hint: torch has a function similar to numpy.random.randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe726aa2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "444f34123df18ddd667c77e74598b96b",
     "grade": false,
     "grade_id": "VAE_class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim : int = 350, embedding_dim : int = 30):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.fc_input = nn.Sequential(nn.Linear(input_dim, 500),\n",
    "                    nn.LeakyReLU(negative_slope=0.2),\n",
    "                    nn.Dropout(0.4)\n",
    "                    )\n",
    "        self.fc_mean = nn.Linear(500, embedding_dim)\n",
    "        self.fc_logstd = nn.Sequential(nn.Linear(500, embedding_dim),\n",
    "                                    nn.LeakyReLU(negative_slope=0.05)\n",
    "                                   )\n",
    "        \n",
    "        self.decoder = nn.Sequential(nn.Linear(embedding_dim, 500),\n",
    "                    nn.LeakyReLU(negative_slope=0.2),\n",
    "                    nn.Dropout(0.4),\n",
    "                    nn.Linear(500, input_dim)\n",
    "                    )\n",
    "\n",
    "    def encode(self, x):\n",
    "        '''\n",
    "        The encoder is composed of a first fully connected layer followed by \n",
    "        two parallel linear layers that generate independently the mean and logstd of the latent distribution\n",
    "        '''\n",
    "        temp = None\n",
    "        mean, logstd = None, None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return mean, logstd\n",
    "        \n",
    "    def reparameterize(self, mean, logstd): \n",
    "        '''\n",
    "        Once the latent distribution is computed from the input a latent vector has to be sampled from it.\n",
    "        We are working on gaussians, look at the formula in the block above \n",
    "        and remember that the encoder produces the logstd.\n",
    "        '''\n",
    "        std = None\n",
    "        eps = None\n",
    "        z = None\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        VAE computes the latent distribution associated with the input, \n",
    "        samples a vector from it and finally decodes it.\n",
    "        '''\n",
    "        mean, logstd = None, None\n",
    "        z = None\n",
    "        out = None \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return [out, mean, logstd, z]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b8399",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1168ebed4fbd548a4853ad5bd29f81b",
     "grade": true,
     "grade_id": "encode",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# encode - 1 point\n",
    "\n",
    "vae = VAE(1,2)\n",
    "m, s = vae.encode(torch.Tensor([1]))\n",
    "assert m.shape==s.shape and m.shape==torch.Size([2]), 'error in encodings dimension'\n",
    "\n",
    "# other hidden tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822602be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a816a4b30e24e4d9eacc2cd4f6ca9bb0",
     "grade": true,
     "grade_id": "reparameterize",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# reparametrize - 1 point\n",
    "\n",
    "vae = VAE(1,2)\n",
    "s = vae.reparameterize(torch.Tensor([0,0]), torch.log(torch.tensor([1,1])))\n",
    "assert s.size()==torch.Size([2]), 'error in dimension of generated random vector'\n",
    "\n",
    "# other hidden tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21c6f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35a47b6859f6f71ed6409e305aeb4c05",
     "grade": true,
     "grade_id": "forward",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# forward - 1 point\n",
    "\n",
    "# hidden tests... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05710adc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db362f68672a4f7f96b070d04ad6eeff",
     "grade": false,
     "grade_id": "cell-ce455d89657ee247",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What we want to maximize in variational Bayes methods is the ELBO, which stands for \"evidence lower bound\":\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "ELBO(\\theta, \\phi) = [\\mathbb{E}_{q_{\\phi}(z)}[\\log p_{\\theta}(x, z) - \\log q_{\\phi}(z|x)] ] = \\\\\n",
    "-KL[q_{\\phi}(z|x)|| p_{\\theta}(z)] + \\frac{1}{L} \\sum_{l=1}^{L} \\log p_{\\theta}(x|z^{(l)})\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where $q_{\\phi}(z|x)$ represents the encoder and $\\log p_{\\theta}(x|z^{(l)})$ is the loglikelihood of the reconstruction and depends on the decoder parameters.\n",
    "\n",
    "The KL (Kullback–Leibler) divergence is a measure of how similar two probability distributions are (if equal -> KL=0) and it is also known as relative entropy. Note that the KL divergence is not a distance metric since it is not symmetric.\n",
    "\n",
    "The KL divergence of some families of distributions can be computed analytically and that's the case if we choose Gaussians priors and posteriors.\n",
    "\n",
    "Implement the KL divergence between a Gaussian posterior $q_{\\phi}(z|x) = \\mathcal{N}(\\mu, \\Sigma)$, where $\\Sigma = diag(\\sigma_{j})$ with $j \\in {1, ..., J}$, and a standard Gaussian prior $p(z) = \\mathcal{N}(0, I)$ : &emsp; $-\\frac{1}{2} \\sum_{j=1}^{J} (1 + \\log((\\sigma_{j})^{2}) - (\\mu_{j})^{2} - (\\sigma_{j})^2)$\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<img src=\"vae.png\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e88cf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2479cff193033f4ad893554d42d4e26",
     "grade": false,
     "grade_id": "KL",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def KL_normnorm(mean, logstd):\n",
    "    '''\n",
    "    implement the closed form of KL divergence for a gaussian as posterior and a standard gaussian as prior\n",
    "    '''\n",
    "    kl = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c60fc7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6e114ea779c9da0ce1d8a2dc1e6d563",
     "grade": true,
     "grade_id": "kl_divergence",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# kl_divergence - 1 point\n",
    "\n",
    "mean = torch.randn(3)\n",
    "logstd = abs(torch.randn_like(mean))\n",
    "try:\n",
    "    KL_normnorm(mean, logstd).item()\n",
    "except:\n",
    "    print('KL divergence of two probability distributions is a number, not a vector...')\n",
    "\n",
    "# other hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75da9ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c809acb227cbd2e3db959c8b5c6fb02b",
     "grade": false,
     "grade_id": "cell-aed77beda024b9ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Differently from the original formulation of VAE we are going to multiply the KL divergence with a scalar parameter in such a way to regulate the tradeoff between the two terms of the ELBO loss (this is somehow similar to beta-VAE: https://openreview.net/references/pdf?id=Sy2fzU9gl). Then, as suggested in https://arxiv.org/pdf/1511.06349.pdf, we will initially set this parameter to zero and increase it linearly w.r.t. the number of epochs (this approach is termed KL warmup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14194fa9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecd449ea2c635a0e5adeb0d9106790f2",
     "grade": false,
     "grade_id": "cell-2a70a73e22bf68ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class VAE_Trainer(AbstractTrainer):\n",
    "    def __init__(self, model : nn.Module, \n",
    "                 train_loader : Optional[DataLoader] = None, \n",
    "                 val_loader : Optional[DataLoader] = None,\n",
    "                 test_loader : Optional[DataLoader] = None, \n",
    "                 optimizer = None):\n",
    "        super().__init__(model=model,\n",
    "                         train_loader = train_loader, \n",
    "                         val_loader = val_loader, \n",
    "                         test_loader = test_loader, \n",
    "                         optimizer = optimizer,\n",
    "                         kl_scaling = 0.1)\n",
    "        self.measures = {'loss' : [], 'reconstr' : []}\n",
    "\n",
    "    def _loss(self, target, label, *args):\n",
    "        out = args[0] \n",
    "        mean = args[1]\n",
    "        logstd = args[2]\n",
    "        batch_dim = target.shape[0]\n",
    "        reconstr_error = self.reconstr_loss(out, target).sum()/batch_dim\n",
    "        kl = KL_normnorm(mean, logstd)/batch_dim\n",
    "        loss = reconstr_error + kl * self.kl_scaling * (self.ep/self.epochs) # kl * scaling * warmup_term\n",
    "        return [loss, reconstr_error]\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        reconstr_errors = []\n",
    "        with torch.no_grad():\n",
    "            for batch in self.train_loader:\n",
    "                data = batch['data'].to(self.device)\n",
    "                out, mean, logstd, z = self.model(data)\n",
    "                reconstr_error = self.reconstr_loss(out, data).sum(-1)\n",
    "                reconstr_errors.append(reconstr_error)\n",
    "        reconstr_errors = torch.cat(reconstr_errors)\n",
    "        thresh = torch.quantile(reconstr_errors, 0.85)\n",
    "        print(f'reconstr_errors avg : {reconstr_errors.mean()}, thresh : {thresh}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(reconstr_errors.numpy(), bins=np.arange(0, 15, 0.125))\n",
    "        ax.set_title('train')\n",
    "        ax.axvline(x=thresh, c='red')\n",
    "        fig.show()\n",
    "\n",
    "        reconstr_errors = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in self.test_loader:\n",
    "                data = batch['data'].to(self.device)\n",
    "                label = batch['labels'].to(self.device).squeeze()\n",
    "                out, mean, logstd, z = self.model(data)\n",
    "                reconstr_error = self.reconstr_loss(out, data).sum(-1)\n",
    "                reconstr_errors.append(reconstr_error)\n",
    "                labels.append(label)\n",
    "        reconstr_errors = torch.cat(reconstr_errors)\n",
    "        labels = torch.cat(labels)\n",
    "\n",
    "        outliers_error = reconstr_errors[labels==1]\n",
    "        normal_error = reconstr_errors[labels==0]\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "        ax1.hist(normal_error.numpy(), bins=np.arange(0, 15, 0.125))\n",
    "        ax1.set_title('normal')\n",
    "        ax1.axvline(x=thresh, c='red')\n",
    "        ax2.hist(outliers_error.numpy(), bins=np.arange(0, 15, 0.125))\n",
    "        ax2.set_title('anomalies')\n",
    "        ax2.axvline(x=thresh, c='red')\n",
    "        fig.show()\n",
    "        print(f' avg recontruction error \\n \\t normal data : {normal_error.mean()}, anomalies : {outliers_error.mean()}')\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        preds = (reconstr_errors>thresh).float()\n",
    "        cm = confusion_matrix(labels.numpy(), preds.numpy(), normalize='true')\n",
    "        sb.heatmap(cm, square=True, xticklabels=False, yticklabels=False, \n",
    "                    annot=True, cbar=False, cmap=\"Reds\", fmt='.2f').get_figure()\n",
    "        ax.set_title('Anomaly detection: confusion matrix')\n",
    "        fig.show()\n",
    "        \n",
    "        ##compute precision, recall and f1-score\n",
    "        # precision = true_positive/(true_positive + false_positive)\n",
    "        # recall = true_positive/(true_positive + false_negative)\n",
    "        true = labels==1            \n",
    "        tp = preds[true].sum()      # true_positive\n",
    "        pos = (preds==1).sum()      # (true_positive + false_positive)\n",
    "        true = true.sum()           # (true_positive + false_negative)\n",
    "        \n",
    "        precision = tp/pos\n",
    "        recall = tp/true\n",
    "        F1_score = 2*(precision * recall)/(precision + recall)\n",
    "        \n",
    "        print(f'Quality measures of our classifier:\\n\\t precision : {precision}\\n\\t recall : {recall}\\n\\t F1_score : {F1_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17e447",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1ae748e48c43c5a1dfba0a673f979d0",
     "grade": false,
     "grade_id": "cell-2b5deef761410602",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will now train our network on the 'normal' ECGs for a cycle of 100 epochs, without using any early stopping criteria, with a scaling constant of 0.1 for the KL divergence and using KL warmup. We will then use our VAE as anomaly detector on a test set composed of an equal (more or less) number of 'normal' and 'PVC' ECGs. \n",
    "\n",
    "To do so we will compute the distribution of the reconstruction errors over the training set, we will get the 0.85 (arbitrarily) quantile of the distribution and use it as a threshold to classify unseen data as belonging or not to the 'normal' class. \n",
    "\n",
    "Graphs produced are:\n",
    "1. summary of training\n",
    "2. few examples of reconstruction of test data\n",
    "3. distribution of reconstruction errors on training set and 0.85 quantile in red\n",
    "4. distribution of reconstruction errors on test set, separately for the 2 classes (threshold in red)\n",
    "5. confusion matrix associated to our detector\n",
    "\n",
    "Three measures were computed to evaluate our classifier: precision, recall and F1 score. Check the code above to see how these are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f1b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(66)\n",
    "torch.manual_seed(66)\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data()\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "trainer = VAE_Trainer(model = model,\n",
    "                      train_loader = train_loader, \n",
    "                      val_loader = val_loader, \n",
    "                      test_loader = test_loader, \n",
    "                      optimizer = optimizer)\n",
    "\n",
    "model = trainer.train(plot=True)\n",
    "plot_reconstr(model, test_loader)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05cd002",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e0e7c798d85b07fb8909c9cf0db3c0d",
     "grade": false,
     "grade_id": "cell-2dc0541786fbbb9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "A vantage of having a probabilistic representation of the encodings is that we can extract uncertainties over the reconstructed data. In the following cell we will perform a simple Monte Carlo evaluation of the uncertainties and plot confidence intervals around the mean reconstruction of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ecaf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c256f339fa293199112c1afcb3e5eae0",
     "grade": false,
     "grade_id": "cell-ef2fbb66719b82f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "data = next(iter(train_loader))['data'][0]\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True)\n",
    "ax[0].plot(np.arange(350), data)\n",
    "\n",
    "model.eval()\n",
    "mean, logstd = model.encode(data)\n",
    "reconstructed_l = []\n",
    "for i in range(10):\n",
    "    z = model.reparameterize(mean, logstd)\n",
    "    reconstructed_l.append(model.decoder(z).unsqueeze(0))\n",
    "reconstructed = torch.cat(reconstructed_l, axis=0)\n",
    "mean = reconstructed.mean(axis=0).detach().numpy()\n",
    "std = reconstructed.std(axis=0).detach().numpy()\n",
    "ax[1].plot(np.arange(350), mean)\n",
    "ax[1].fill_between(np.arange(350), mean + 2*std, mean - 2*std, alpha=0.2, edgecolor='blue')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d523f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70accb14463699ad1b18e4c40c6cff36",
     "grade": false,
     "grade_id": "cell-31b96ff5858f1063",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "VAEs can be used to generate new datapoints. Once the network has been trained, we can sample latent representations from the prior distribution (in our case a standard gaussian) and decode them to produce new unseen data. The same mechanism is at the base of GANs. \n",
    "\n",
    "Try to produce new data yourself (the box is not graded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1f5ba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79093589f1ebece951b2dc2f3b5cfde6",
     "grade": false,
     "grade_id": "cell-52da14748ffd327a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "size = model.embedding_dim\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True)\n",
    "for i in range(6):\n",
    "    sample = None           # sample from the prior (standard normal)\n",
    "    new_data = None         # feed the sample to the decoder to produce new data\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # if you get an error in the plot function, just have a look at how plots were produced in the block above\n",
    "    axs[i%2,i%3].plot(np.arange(350), new_data)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106cd2a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccf497965f5c1fb10bd9c96c7aa1a655",
     "grade": false,
     "grade_id": "cell-c641a99949382c50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have seen the basic structure of a VAE, tested it on a anomaly detection task and seen how new data can be generated by means of the decoder. We will now see how we can enforce simple properties on the representations in the latent space.\n",
    "\n",
    "By minimizing the KL we are forcing the representations (remember that these are distributions, not single values) produced by the encoder to be as close as possible to a standard normal. We could go beyond this and impose, for example, that the distributions of representations corresponding to different classes of data occupy different volumes of the latent space.\n",
    "\n",
    "Our training set will now consist of ECGs of both 'normal' and 'PVC' classes, we will train, in parallel to the VAE, a linear classifier on the means of the distributions produced by the encoder. In this way the means of the distributions corresponding to different classes will be pushed in linearly separable volumes of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17950592",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d279f61f0e03fd865de48091bb857bc0",
     "grade": false,
     "grade_id": "cell-f4c9b1cc4a2be9d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement a simple classifier of one single fully connected layer. Select the right activation function for a binary classifier (you have already encountered it in a previous exercise).\n",
    "\n",
    "Hint: torch.nn module has useful functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b4ada",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d832ae83c3f149db42140d6796675b6",
     "grade": false,
     "grade_id": "Composed_NN",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Composed(nn.Module):\n",
    "    def __init__(self, input_dim : int = 350, embedding_dim : int = 3):\n",
    "        super().__init__()\n",
    "        self.vae = VAE(input_dim, embedding_dim)\n",
    "        self.classifier = BinaryClassifier(embedding_dim)\n",
    "    def forward(self, data):\n",
    "        out, mean, logstd, z = self.vae(data)\n",
    "        logits = self.classifier(mean).squeeze()\n",
    "        return [out, mean, logstd, z, logits]\n",
    "\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = None           # f(ully)c(onnected)\n",
    "        self.activation = None   # we need a differentiable function between 0 and 1... \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.activation(self.fc(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a86d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51e3684ffb037b6b6e0f167da13fc8c5",
     "grade": true,
     "grade_id": "Binary_classifier",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Binary_classifier - 1 point\n",
    "\n",
    "# hidden tests (checking nn.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5459bc3",
   "metadata": {},
   "source": [
    "Try to implement the loss of our new architecture. It will have to be composed by 3 additive terms: the likelihood for the reconstruction, the kl divergence for the prior and a term for the binary classifier. \n",
    "\n",
    "(For this task you may drop the KL warmup)\n",
    "\n",
    "Hints: \n",
    "1. maybe you want to take a look at the VAE_Trainer loss function implemented above...\n",
    "2. ComposedTrainer class has a new attribute. What is the BCELoss? \n",
    "3. remember that you already implemented a function for the KL divergence \n",
    "4. have you asked yourself what is the meaning of \"reduction='none'\" in the torch.nn functions we used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30588da",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "851f9ec57af0d7505163513f47096d06",
     "grade": false,
     "grade_id": "cell-c199db04cd7f601a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ComposedTrainer(AbstractTrainer):\n",
    "    def __init__(self, model : nn.Module, \n",
    "                 train_loader : Optional[DataLoader] = None, \n",
    "                 val_loader : Optional[DataLoader] = None,\n",
    "                 test_loader : Optional[DataLoader] = None, \n",
    "                 optimizer = None):\n",
    "        super().__init__(model=model,\n",
    "                         train_loader = train_loader,\n",
    "                         val_loader = val_loader,\n",
    "                         test_loader = test_loader,\n",
    "                         optimizer = optimizer,\n",
    "                         early_stop = 2)\n",
    "        self.class_loss = nn.BCELoss(reduction='none')       \n",
    "        self.measures = {'loss' : [], 'reconstr' : [], 'classif' : []}\n",
    "\n",
    "    def _loss(self, target, label, *args):\n",
    "        out = args[0] \n",
    "        mean = args[1]\n",
    "        logstd = args[2]\n",
    "        logits = args[4]\n",
    "        batch_dim = target.shape[0]\n",
    "        #                               <============================ THE FOLLOWING 4 LINES ONLY\n",
    "        reconstr_error = None      # an old friend\n",
    "        kl = None                  # another old friend\n",
    "        classif_error = None       # a new one\n",
    "        loss = None                \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return [loss, reconstr_error, classif_error]\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        means = []\n",
    "        labels = []\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                data = batch['data'].to(self.device)\n",
    "                label = batch['labels'].to(self.device).squeeze()\n",
    "                out, mean, logstd, z, logits = self.model(data)\n",
    "                means.append(mean)\n",
    "                labels.append(label)\n",
    "                preds.append((logits>0.5).float())\n",
    "        labels = torch.cat(labels).numpy().astype('int')\n",
    "        preds = torch.cat(preds).numpy().astype('int')\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        cm = confusion_matrix(labels, preds, normalize='true')\n",
    "        sb.heatmap(cm, square=True, xticklabels=False, yticklabels=False, \n",
    "                    annot=True, cbar=False, cmap=\"Reds\", fmt='.2f').get_figure()\n",
    "        ax.set_title('Classification: confusion matrix')\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "        means = torch.cat(means).numpy()\n",
    "        colors={0: 'blue', 1:'red'}\n",
    "        fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "        ax.scatter(means[:,0], means[:,1], means[:,2], marker=\".\", s=20, c=[colors[i] for i in labels])\n",
    "        ax.set_title('Latent space')\n",
    "        fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901123a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf03e5a1ebf4d6cfd5d35983eb50e1c1",
     "grade": true,
     "grade_id": "Composed_Loss",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Composed_Loss - 1 point\n",
    "\n",
    "# hidden tests... did you check the hints?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce8f11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "363c140e48b8b6de5e27814d219a4093",
     "grade": false,
     "grade_id": "cell-c298835b1ae392b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will now train our network for a cycle of 100 epochs, using as early stopping criteria the classification error on the validation set, with a scaling constant of 0.5 for the KL divergence and using (or not) KL warmup. \n",
    "\n",
    "Graphs produced are:\n",
    "1. summary of training\n",
    "2. confusion matrix associated to the binary classifier\n",
    "4. interactive 3D graph in the latent space of the means of the representation distributions produced by the encoder on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f668c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(666)\n",
    "torch.manual_seed(666)\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data(composed=True)\n",
    "model = Composed()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "trainer = ComposedTrainer(model = model,\n",
    "                          train_loader = train_loader, \n",
    "                          val_loader = val_loader, \n",
    "                          test_loader = test_loader, \n",
    "                          optimizer = optimizer)\n",
    "\n",
    "model = trainer.train(plot=True)\n",
    "#     plot_reconstr(model, test_loader)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b4d304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
