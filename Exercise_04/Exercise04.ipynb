{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbc32aa52bb9e2d1c1e435b7c692ec1b",
     "grade": false,
     "grade_id": "cell-23f9a13fcbe0410a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise Sheet No. 4\n",
    "\n",
    "---\n",
    "\n",
    "> Machine Learning for Natural Sciences, Summer 2021, Jun.-Prof. Pascal Friederich, pascal.friederich@kit.edu\n",
    "> \n",
    "> Deadline: 10.05.2021, 8am\n",
    "\n",
    "---\n",
    "\n",
    "**Topic**: This exercise sheet will focus on the math basics for ML.\n",
    "\n",
    "**[Before you start, please fill out our survey for the exam!](https://forms.gle/poPp6yY8TxHcPv3E7)**\n",
    "\n",
    "We will use seaborn as a more abstract plotting interface in this exercise. You can install it into your current conda environment by executing the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "19cf46e1f5bdbd5e2976ccb33476ac69",
     "grade": false,
     "grade_id": "cell-6754d0c5a478283c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Linear Regression\n",
    "In assignment 2, we have seen the use of decision tree for iris species classification. Now, let's try to solve the same problem with another simple machine learning technique: linear regression (LR).   \n",
    "\n",
    "Linear regression uses a linear combination of features to predict a target. In our case we can use a linear combination of the four flower descriptors to predict the species.\n",
    "\n",
    "In this assignment, you will learn to implement the Loss function and Gradient Decent for optimization. Then, we will see that even small model like LR can overfit, and how regularization (ridge regression) can help against this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import random\n",
    "from functools import wraps\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c85a1f4f71c05ff0d236a0fed190d64f",
     "grade": false,
     "grade_id": "cell-1cb644d0633a97e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "Let's start with preprocessing the dataset, as already learned from assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_name(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"\\n{func.__name__}:\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def log_shape(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"\\tshape: {result.shape}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def log_columns(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"\\tcolumns: {result.columns.values}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@log_columns\n",
    "@log_shape\n",
    "@log_name\n",
    "def load(df, path):\n",
    "    \"\"\"Loads the dataset from path.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "@log_columns\n",
    "@log_shape\n",
    "@log_name\n",
    "def convert_to_categorical(df, col_name: str):\n",
    "    df[col_name] = df[col_name].astype('category')\n",
    "    return df\n",
    "\n",
    "@log_columns\n",
    "@log_shape\n",
    "@log_name\n",
    "def add_class_labels(df):\n",
    "    df['class'] = df['species'].cat.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = (\n",
    "    df.pipe(load, 'iris.csv')\n",
    "      .pipe(convert_to_categorical, 'species')\n",
    "      .pipe(add_class_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10f7e894cc35f0afceeca6333f158367",
     "grade": false,
     "grade_id": "cell-0b250c3f733fc1f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Plot the data\n",
    "It is always a good practice to have some inspection on the dataset. This provides us with information about the data such as its structure, range, outliers etc., which may help on the design of ML algorithm. There are numerous ways to visualize data information, and introduced here is the one called \"Pairplot\", which displays pairwise relationships in a dataset. Pairplot can be implemented easily with the [seaborn](https://seaborn.pydata.org/generated/seaborn.pairplot.html?highlight=pairplot#seaborn.pairplot) library. Here we used it to show the pairwise relationships among iris features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.loc[:,:'species'], hue= 'species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02d10a133b459846ebd3dbf3be223495",
     "grade": false,
     "grade_id": "cell-d45fb09151e8b752",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Question: out of these combinations of features, which pair has the strongest linear correlation relationship?\n",
    "\n",
    "**1.** sepal_length and petal_width    \n",
    "**2.** sepal_width and petal_length    \n",
    "**3.** petal_length and petal_width    \n",
    "**4.** sepal_width and petal_length    \n",
    "**5.** sepal_length and sepal_width\n",
    "\n",
    "(To learn more about the linear correlation, please refer to [here](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient))\n",
    "\n",
    "Assign the number of your choice to the variable `A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3380f36e81954d7d0f7f6121153c2f21",
     "grade": false,
     "grade_id": "cell-a0ba3944ddcc7de8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "A = (\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c95cdab8d13c7f179ec34831082809e7",
     "grade": true,
     "grade_id": "Correlation_Plot-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf508cff095bf97f30df88ea7d77e287",
     "grade": false,
     "grade_id": "cell-ebcde626c67fdc9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Generate training/validation set\n",
    "Now, for our machine learning task, let's extract feature matrix `X` and label matrix `Y` from the dataframe `df`. Please implement the `split_X_Y` function that returns feature `X` as a $150 \\times 4$ numpy array, as well as label `Y` as a $150 \\times 1$ numpy array. `X` should contain values from column \"sepal_length\", \"sepal_width\", \"petal_length\" and \"petal_width\". `Y` should be the value of column \"class\". You may use methods such as `.reshape()` etc., and attributes like `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8634e7e480492b2e7c423eed7f137976",
     "grade": false,
     "grade_id": "cell-d5f1768efcd1cdd0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def split_X_Y(df):\n",
    "    \"\"\"split the dataframe into feature matrix X and label matrix Y\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b9f736b5bd21244cf06795b7cc4101b",
     "grade": true,
     "grade_id": "Train_Test_Split-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X, Y = split_X_Y(df)\n",
    "assert X.shape == (150, 4)\n",
    "assert Y.shape == (150, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10f8af35e22442d6ee80fc9f64df5dea",
     "grade": false,
     "grade_id": "cell-af56efe9d9027cdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have learned that the linear model tries to fit the function:\n",
    "\\begin{align}\n",
    "y = \\omega^T x + \\omega_0 = \\sum^n_{i=1} \\omega_i x_i + \\omega_0\n",
    "\\end{align}\n",
    "Where $\\omega$ is the weight vector and $\\omega_0$ is the bias term. Let $x_0 = 1$, this function can be rewritten into:\n",
    "\\begin{align}\n",
    "y =  \\sum^n_{i=1} \\omega_i x_i + \\omega_0 x_0 = \\sum^n_{i=0} \\omega_i x_i = X \\Omega^T\n",
    "\\end{align}\n",
    "$\\Omega$ can be initialized randomly. For our case it has five element - the first for the bias and four for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "omega = np.random.randn(1,5)\n",
    "omega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fcc2e52f1d9230e0cb66142c8f14455",
     "grade": false,
     "grade_id": "cell-75bfd7154053f864",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To contract the bias term and the features to obtain $X$, we can stack a column to feature matrix `X` with all values equal to $1$. This may be easily implemented with the numpy method `.hstack()` . You may find more details [here](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5ce3d3f98bc1b46afba5a310113da86",
     "grade": false,
     "grade_id": "cell-ae603908925fe802",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = (\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7005cfaa57889b3830f4829cd3850b6a",
     "grade": true,
     "grade_id": "Add_bias_term_to_X-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert X.shape == (150, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbf8dbec7af76a123e9a4c25dab38652",
     "grade": false,
     "grade_id": "cell-3063ec8d8014755b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There's a \"closed-form solution\" that estimates the best parameter set $\\Omega^*$ by solving the equation:\n",
    "\\begin{align}\n",
    "\\Omega^* = (X^TX)^{-1}X^Ty\n",
    "\\end{align}\n",
    "Watch this [video](https://www.coursera.org/lecture/ml-regression/approach-1-closed-form-solution-G9oBu) from coursera for detailed explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4282edbdcc30b9945a729f6d28e33968",
     "grade": false,
     "grade_id": "cell-3b6704681c9de592",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Gradient Decent\n",
    "For more complex tasks a closed form solution often doesn't exist and hence we will introduce another approach, aka Gradient Decent (GD) algorithm, to solve the linear regression. Gradient Decent works by updating the weight vector incrementally after each epoch. Read more [here](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "433acc61190ded17051ff62d3b8e97fb",
     "grade": false,
     "grade_id": "cell-2aac0b9bb5d8eb89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Firstly, let's do a train-valid split for our dataset, as we already saw in Assignment 2. The `x_train` and `y_train` will be our training set, and `x_val`, `y_val` will be our validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "681f7882a90fe42a2be5052717b87ce7",
     "grade": false,
     "grade_id": "cell-08de0d05761681ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There's also a useful method `sklearn.model_selection.train_test_split()` from scikit-learn for the same task. You can find its documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = list(range(0, 150))\n",
    "random.shuffle(idx)\n",
    "x_train = X[idx[:120]]\n",
    "y_train = Y[idx[:120]]\n",
    "x_val = X[idx[120:]]\n",
    "y_val = Y[idx[120:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0829340c8b617367aeb1dd9c3fe2de87",
     "grade": false,
     "grade_id": "cell-a0ed23a25e5d3cd1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Cost function\n",
    "The mean squared error is a common loss function that measures the average squared difference between predict and real values. Here, it is defined as:\n",
    "\\begin{align}\n",
    "MSE = \\frac{1}{2n}\\sum^n_{i=1}(\\text{y_pred}_i - \\text{y_real}_i)^2\n",
    "\\end{align}\n",
    "The $\\frac{1}{2}$ in equation is just for convenience when computing the gradient (see next step). Recall that $y = X \\Omega^T$, so we have:\n",
    "\\begin{align}\n",
    "MSE = \\frac{1}{2n}\\sum^n_{i=1}(x_i \\Omega^T - \\text{y_real}_i)^2\n",
    "\\end{align}\n",
    "Please implement the `mean_square_error()` that returns MSE. Methods/attribute you may need are `numpy.sum()`, `numpy.dot()` and `ndarray.T`. Although we use a $\\sum$ in the notation you can use your knowledge from exercise 3 to compute the following using only numpy vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63348d99f845fa7b05f13e4f986bfdbf",
     "grade": false,
     "grade_id": "cell-2fd989d64b2d9018",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_square_error(x, y, omega):\n",
    "    \"\"\"\n",
    "    return the mean suqared error.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of features.\n",
    "        y: numpy array of corresponding classes.\n",
    "        omega: weight vector.\n",
    "        \n",
    "    Returns:\n",
    "        mse: mean squared error.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c798ac17654f702d3de3b2a984584c04",
     "grade": true,
     "grade_id": "MSE-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[1.0, 2.0], [2.0, 2.0]])\n",
    "b = np.array([[1.0], [2.0]])\n",
    "w = np.array([[1.0, 1.0]])\n",
    "assert mean_square_error(a, b, w) == 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30a5191eacd8ac490290e82e019244c1",
     "grade": false,
     "grade_id": "cell-0c61b6eda0caba8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After each epoch $t$, the Gradient Decent algorithm updates the weight vector in the direction of the negative gradient in order to reduce the cost function. The gradient is simply the partial derivative of mean squared error to the weight.\n",
    "\n",
    "You can deduct the derivative yourself for practice. Just run the following cell to render the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"\\\\begin{align}\"\n",
    "                 \"\\\\frac{\\\\partial MSE}{\\\\partial \\\\Omega} &= \"\n",
    "                 \"\\\\frac{1}{n} \\\\sum^n_{i=0}(x_i \\\\Omega^T - \\\\text{y_real}_i) x_i \\\\\\\\\"\n",
    "                 \"\\\\Omega(t+1) &= \\\\Omega(t) - \\\\alpha \\\\frac{\\\\partial MSE}{\\\\partial \\\\Omega(t)}\"\n",
    "                 \"= \\\\Omega(t) - \\\\frac{\\\\alpha}{n} \\\\sum^n_{i=0}(x_i \\\\Omega^T - \\\\text{y_real}_i) x_i\"\n",
    "                 \"\\\\end{align}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4fffb8553581d0b673857ccefddce13",
     "grade": false,
     "grade_id": "cell-3b243d3fd3dba44a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The $\\alpha$ is the learning rate that controls the step size for the update after each iteration.\n",
    "\n",
    "Please implement the `weight_update_function()` that returns updated $\\Omega$. Methods/attribute you may need are `numpy.dot()`, `numpy.reshape()` and `ndarray.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "479d9059a476ea990295d1619cbdcb21",
     "grade": false,
     "grade_id": "cell-020ba1d7af87bf88",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weight_update_function(x, y, omega, alpha):\n",
    "    \"\"\"\n",
    "    return updated set of weights\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of features.\n",
    "        y: numpy array of corresponding classes.\n",
    "        omega: weight vector.\n",
    "        alpha: learning rate.\n",
    "        \n",
    "    Returns:\n",
    "        omega_updated: the updated weight vector.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return omega_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24f6a7df3ec0ebd3a86680e018633e71",
     "grade": true,
     "grade_id": "Gradient_Descent-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(np.mean(weight_update_function(a, b, w, 0.001)) - 0.996) <= 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ad4477da5509789e250ef8e37d83616",
     "grade": false,
     "grade_id": "cell-7425ef1050a146d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To monitor the training process, here we use mean absolute error:\n",
    "\\begin{align}\n",
    "MAE = \\frac{1}{n}\\sum^n_{i=1} |\\text{y_pred}_i - \\text{y_real}_i|\n",
    "\\end{align}\n",
    "Please implement the `mean_absolute_error()` that returns MAE. Methods/attribute you may need are `numpy.sum()`, `numpy.dot()`, `numpy.abs()` and `ndarray.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43acd60216626e1172f42de3240e6eb0",
     "grade": false,
     "grade_id": "cell-5b77b93b9896b0aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_absolute_error(x, y, omega):\n",
    "    \"\"\"\n",
    "    return the mean absolute error\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of features.\n",
    "        y: numpy array of corresponding classes.\n",
    "        omega: weight vector.\n",
    "        \n",
    "    Returns:\n",
    "        mse: mean absolute error.\n",
    "        \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb28ea2061b69ae54e0c92d3bce6d7a2",
     "grade": true,
     "grade_id": "MAE-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.round(mean_absolute_error(a, b, w))  == 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33b4c8e7becd1437f1738230361e597a",
     "grade": false,
     "grade_id": "cell-851877d35d2dd051",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the last step before training the model, let's set up the learning rate and number of iterations. We use `J_train` and `J_val` to record mean absolute errors of the training and validation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000 # number of updates to the weight\n",
    "alpha = 0.01 # learning rate\n",
    "J_train = np.zeros(epochs) # MAE of training process\n",
    "J_val = np.zeros(epochs) # MAE of validation process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4845353412e04f0a9d0323ed212329f4",
     "grade": false,
     "grade_id": "cell-13d06de7e4e3c1f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training & plot\n",
    "Now, let's train our model with the `weight_update_function()` and record mean absolute errors through `mean_absolute_error()` for the training/validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77fd7342b62823d47dceb2379363624e",
     "grade": false,
     "grade_id": "cell-f22a7bd148ab6b24",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "omega = np.random.randn(1,5)\n",
    "for i in range(epochs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8dce916af6b0d2df7820d04ffdd9174",
     "grade": true,
     "grade_id": "Train_Loop-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert J_train[-1] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71045f025936bda70861448c17ad0a90",
     "grade": true,
     "grade_id": "Train_Loop-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9355fea4e3327f217dcc560b627f4b45",
     "grade": false,
     "grade_id": "cell-ae74de811a5efbf3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here the `plot_training_curve()` is implemented to visualize the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curve(MAE_train, MAE_val, epochs):\n",
    "    \"\"\"Plot the mean absolute error for training/validation process\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "    ax.plot(np.arange(epochs), MAE_train, label='Training')\n",
    "    ax.plot(np.arange(epochs), MAE_val, label='Validation')\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_ylabel(\"Mean Absolute Error\")\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_title(\"Mean Absolute Error vs Epochs\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curve(J_train, J_val, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "402599538f41fc8b5d276ed78102f61a",
     "grade": false,
     "grade_id": "cell-d1d5ee5a2c54fcc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Accuracy\n",
    "Now, with the optimized weight vector, let's implement `prediction()` to get the predicted class labels from our linear model. You may need `numpy.dot()` and `ndarray.T` for the computation, and `numpy.round()` to round up the results into integers (because our class labels are integer 0, 1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed00a721040b8f66b1f1e01c58ae453c",
     "grade": false,
     "grade_id": "cell-3dcb65242819c457",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def prediction(X, omega):\n",
    "    \"\"\"\n",
    "    Return predicted labels.\n",
    "    \n",
    "    Args:\n",
    "        X: the feature matrix.\n",
    "        omega: optimized weight vector\n",
    "        \n",
    "    Returns:\n",
    "        Y_pred: predicted labels.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0ca3a1410d36856305c55aceb7f783b",
     "grade": true,
     "grade_id": "Accuracy-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Y_pred = prediction(X, omega)\n",
    "assert Y_pred.shape == Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be7cae7f617b829607202be3cba95c33",
     "grade": false,
     "grade_id": "cell-3ff8c071f6a86c8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With `Y_pred`. Please implement `accuracy()` that calculates the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3965a8cad791e76bc698a2b128a8300b",
     "grade": false,
     "grade_id": "cell-1ecb0fa5853735c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(Y_pred, Y):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of prediction.\n",
    "    \n",
    "    Args:\n",
    "        Y_pred: the predicted class labels.\n",
    "        Y: the real class labels.\n",
    "        \n",
    "    Returns:\n",
    "        acc: calculated accuracy in float.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy(Y_pred, Y)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c8944753ba5fa9316d532a58185bd9b",
     "grade": false,
     "grade_id": "cell-71252f5d8826fc60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What's the accuracy of your model? Please input it below (with 4 significant figures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5867a35304591a6f6d446d3308390c37",
     "grade": false,
     "grade_id": "cell-ccf6ea013161689a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "my_accuracy = (\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe664be4180a6f34975faf5e85fe4422",
     "grade": true,
     "grade_id": "Accuracy-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2b70debd6883360e007b365656c4e64",
     "grade": false,
     "grade_id": "cell-cfd646f9a9655148",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Visualize predictions vs ground truth\n",
    "For the last step, let's visualize our predictions v.s. the real labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predict_vs_real(Y_pred, Y):\n",
    "    \"\"\"Plot the predictions vs ground truch\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "    ax.scatter(np.arange(1, 151, 1), Y_pred, label='Predictions')\n",
    "    ax.plot(np.arange(1, 151, 1), Y, label='Ground Truch', color='red')\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.yticks(np.arange(0, 3, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict_vs_real(Y_pred, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67a13473a27f4a26d2b2273f10d48ec6",
     "grade": false,
     "grade_id": "cell-b22ad7087da3354e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also write confusion matrix as assignment 2 to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "810e48f2b0ff722a7d7e6a82c5286584",
     "grade": false,
     "grade_id": "cell-68f7723b4445e9cb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def confusion_table(Y_pred, Y):\n",
    "    \"\"\"Plot the confusion table\"\"\"\n",
    "    confusion_matrix = pd.DataFrame(index=df['species'].unique(),\n",
    "                                columns=df['species'].unique(),\n",
    "                                data = np.zeros((3,3)))\n",
    "    confusion_matrix.index.name = 'True'\n",
    "    confusion_matrix.columns.name = 'Predicted'\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_table(Y_pred, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5d58d8abf85e10044ee33e736da78d4",
     "grade": false,
     "grade_id": "cell-3e8e912f3964f539",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How many points are mis-classified? Please input you answer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec89da1ea98907efac476c4c573c55a7",
     "grade": false,
     "grade_id": "cell-8e4b8a64042ee33f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mis_classified = (\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bb001eaa6f76a9578ae577d8459a97f",
     "grade": true,
     "grade_id": "Confusion_Matrix-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43351c3b8c701d4e617cf84f42900d78",
     "grade": false,
     "grade_id": "cell-165d84e42ab77f0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please feel free to play around with the model and training process. You may adjust the learning rate and number of epochs to see how the trainig curve changes (do not forget to re-initialize the weight vector). You may also try different train-validation split ratio to see the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28746d6c2e44b1f4af8c15eb64e4157c",
     "grade": false,
     "grade_id": "cell-926be4a613646f05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Overfitting & Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8cd31d89c809cdb62e872fdbe6d3f75",
     "grade": false,
     "grade_id": "cell-59d2a351bd90a39f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When finishing this assignment, you may (and may not, depending on the training set setup) sometimes observe the gap between the training curve and validation curve (validation error higher than training error) that cannot be diminished by increasing the number of epochs. This phenomenon is known as \"overfitting\". It happens when the model gets too complex so that it fits the training set perfectly, but loses the generalization ability towards unseen data from validation/test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f85034fb8afc6192217ba975c6bbf64",
     "grade": false,
     "grade_id": "cell-0f409bfd47dc63d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Ridge regression, also known as L2 regularization, is a useful technique to restrict our model from getting too complicated and reduce the effect of overfitting. It works by simply adding a penalty term to the cost function. In our case, the penalty term is $||\\Omega||^2$. It prefers lower absolute values of weights thus reduce the model complexity. For more information, please refer to [here](https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed66f8a45d23d9e0cb18650617766cf7",
     "grade": false,
     "grade_id": "cell-a2fef35e4cc07408",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By introducting the penalty term, our cost function becomes:\n",
    "\\begin{align}\n",
    "MSE = \\frac{1}{2}(\\frac{1}{n}\\sum^n_{i=1}(\\text{y_pred}_i - \\text{y_real}_i)^2 + \\lambda ||\\Omega||^2)\n",
    "\\end{align}\n",
    "$\\lambda$ is the coefficient to adjust the regularization effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23f3ed1ec973078bd45f767eafb964a9",
     "grade": false,
     "grade_id": "cell-6e8209261b08ce72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "While the mean absolute error calculation stays the same, the weight update function becomes:\n",
    "\\begin{align}\n",
    "\\Omega = \\Omega - \\alpha \\frac{\\partial MSE}{\\partial \\Omega} = \\Omega - \\alpha(\\frac{1}{n} \\sum^n_{i=0}(x_i \\Omega^T - \\text{y_real}_i) x_i + \\lambda ||\\Omega||) \n",
    "\\end{align}\n",
    "Now, please implement the new cost function `mean_square_error_ridge()` and `weight_update_function_ridge()` with the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdfc5a5ece064509ddb12f2783431520",
     "grade": false,
     "grade_id": "cell-54d887bbc3b81ff8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_square_error_ridge(x, y, omega, lam):\n",
    "    \"\"\"\n",
    "    return the mean suqared error.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of features.\n",
    "        y: numpy array of corresponding classes.\n",
    "        omega: weight vector.\n",
    "        lam: ridge regression coefficient.\n",
    "        \n",
    "    Returns:\n",
    "        mse: mean squared error.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90b857928ebb065820c3b8ae97cd31fa",
     "grade": true,
     "grade_id": "Ridge_Regression-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[1.0, 2.0], [2.0, 2.0]])\n",
    "b = np.array([[1.0], [2.0]])\n",
    "w = np.array([[1.0, 1.0]])\n",
    "assert np.round(mean_square_error_ridge(a, b, w, 0.1), 1) == 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e47646c4f476d842da89c1d4de941b84",
     "grade": false,
     "grade_id": "cell-a97bb7ee36704ee5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weight_update_function_ridge(x, y, omega, alpha, lam):\n",
    "    \"\"\"\n",
    "    return updated set of weights\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of features.\n",
    "        y: numpy array of corresponding classes.\n",
    "        omega: weight vector.\n",
    "        alpha: learning rate.\n",
    "        lam: ridge regression coefficient.\n",
    "        \n",
    "    Returns:\n",
    "        omega_updated: the updated weight vector.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return omega_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ddc911f833815e54054463c56d2b8d6",
     "grade": true,
     "grade_id": "Ridge_Regression-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(np.mean(weight_update_function_ridge(a, b, w, 0.001, 0.1)) - 0.99) <= 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "228944becea614c63a4c7c5a6d4551e2",
     "grade": false,
     "grade_id": "cell-32d88540dd3063f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's look at a very unbalanced training set. In this example, only 25 instances are used as training set ($17\\%$), and the number of class 0 is significantly higher than both class 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('iris_overfit', 'rb') as f:\n",
    "    x_train, x_val, y_train, y_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(np.arange(0, len(y_train), 1), y_train)\n",
    "plt.yticks(np.arange(0, 3, 1))\n",
    "ax.set_xlabel(\"data index\")\n",
    "ax.set_ylabel(\"class label\")\n",
    "ax.set_title(\"Class distribution of training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09963cd951855227a5e7c42432197740",
     "grade": false,
     "grade_id": "cell-cdee0339af40f024",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the training, let's do the normal linear regression and ridge regression side by side, with `J_train`, `J_val` recording linear regression training/validation MAE, and `J_train_ridge`, `J_val_ridge` recording ridge regression MAE. Please implement them in the same `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "omega = np.random.randn(1,5)\n",
    "epochs = 3000 # number of updates to the weight\n",
    "alpha = 0.0005 # learning rate\n",
    "lam = 0.5 # coefficient of L2 penalty\n",
    "J_train = np.zeros(epochs) # record MAE of training process (without penalty)\n",
    "J_val = np.zeros(epochs) # record MAE of validation process (without penalty)\n",
    "J_train_ridge = np.zeros(epochs) # record MAE of training process (with penalty)\n",
    "J_val_ridge = np.zeros(epochs) # record MAE of validation process (with penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a1d873f82fc1ae2c83d9e634b7f9721",
     "grade": false,
     "grade_id": "cell-b44928bfd37e98e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "omega_norm = np.copy(omega) # weight vector for training without penalty\n",
    "omega_ridge = np.copy(omega) # weight vector for ridge regression\n",
    "\n",
    "for i in range(epochs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9fb9546e45913d84c653e899fc6f896",
     "grade": true,
     "grade_id": "Train_Loop_Ridge-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert J_train_ridge[-1] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95ca05736571b659f345032e321b1e66",
     "grade": true,
     "grade_id": "Train_Loop_Ridge-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b13bdefa45e339152180be31a55445b",
     "grade": false,
     "grade_id": "cell-edcd9970ea4609f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we plot the training curves again, but this time on a log-log scale to better see the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curve_ridge(MAE_train, MAE_val, MAE_train_ridge, MAE_val_ridge, epochs):\n",
    "    \"\"\"Plot the mean absolute error for training/validation process\"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax[0].loglog(np.arange(epochs), MAE_train, label='Training')\n",
    "    ax[0].loglog(np.arange(epochs), MAE_val, label='Validation')\n",
    "    #ax.set_ylim([0,1])\n",
    "    ax[0].set_ylabel(\"Mean Absolute Error\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_title(\"Without L2 Regularization\")\n",
    "    ax[0].legend(loc='upper right')\n",
    "    \n",
    "    ax[1].loglog(np.arange(epochs), MAE_train_ridge, label='Training')\n",
    "    ax[1].loglog(np.arange(epochs), MAE_val_ridge, label='Validation')\n",
    "    #ax.set_ylim([0,1])\n",
    "    ax[1].set_ylabel(\"Mean Absolute Error\")\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_title(\"With L2 Regularization\")\n",
    "    ax[1].legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curve_ridge(J_train, J_val, J_train_ridge, J_val_ridge, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = prediction(X, omega_norm)\n",
    "Y_pred_ridge = prediction(X, omega_ridge)\n",
    "acc = accuracy(Y_pred, Y)\n",
    "acc_ridge = accuracy(Y_pred_ridge, Y)\n",
    "print(acc)\n",
    "print(acc_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict_vs_real(Y_pred, Y)\n",
    "plot_predict_vs_real(Y_pred_ridge, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "daa99a08c6d4cae43e71fac723bb0fdf",
     "grade": false,
     "grade_id": "cell-11f395039356b11f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Yes or No question: from the plots/accuracy calculations, do you think the ridge regression improved the learning performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "270be3c130e807ee159eb0bccd4a2266",
     "grade": false,
     "grade_id": "cell-54e049af8010945b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Answer = (\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "064d95d4980d0064674c95967150560a",
     "grade": true,
     "grade_id": "Train_Loop_Ridge-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5055c03f08fd98b5f02ae7aab78e47f8",
     "grade": false,
     "grade_id": "cell-298117388b266b67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scipy Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06d235aa081fe52b39148d8369007e09",
     "grade": false,
     "grade_id": "cell-1e16607a3299687d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Rather than handcraft the Gradient Decent algorithm, there are libraries that can do the optimization task automatically. The `minimize()` function provided by SciPy library is a good example. It takes as input the name of the function that needs to be minimized, the initial point where the search starts and (optionally) the name of a specific search algorithm. It returns a `OptimizeResult` object that contains details of the optimization result. Below is an example of using `minimize()` function to optimize our linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "np.random.seed(0)\n",
    "omega = np.random.randn(1,5)\n",
    "\n",
    "def mean_square_error(omega):\n",
    "    return (1/(2*len(X)) * np.sum((np.dot(X, omega.reshape(1,5).T) - Y) ** 2 ))\n",
    "\n",
    "\n",
    "res = minimize(mean_square_error, omega, method='L-BFGS-B')\n",
    "print(res)\n",
    "omega_op = np.array(res.x).reshape(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = prediction(X, omega_op)\n",
    "acc = accuracy(Y_pred, Y)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predict_vs_real(Y_pred, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01e3d4819c61e32253fc5ba270d0da7e",
     "grade": false,
     "grade_id": "cell-11e0b1c3d98cf009",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can find a good tutorial of SciPy optimization [here](https://machinelearningmastery.com/function-optimization-with-scipy/).  \n",
    "\n",
    "If you have more questions, please refer to the SciPy [documentation](https://docs.scipy.org/doc/scipy/reference/optimize.html).\n",
    "\n",
    "For the introduction of the \"L-BFGS-B\" search algorithm using here, please read [this](http://sepwww.stanford.edu/data/media/public/docs/sep117/antoine1/paper_html/node6.html).\n",
    "\n",
    "***Please do this in a separate notebook that is not submitted!***\n",
    "\n",
    "**[If you didn't do it in the beginning please fill out the survey for the exam now!](https://forms.gle/poPp6yY8TxHcPv3E7)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
